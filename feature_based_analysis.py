# Proje iÃ§in kullanÄ±lan dosya
# feature_based_analysis.py dosyasÄ±, duygu analizi ve metin sÄ±nÄ±flandÄ±rma iÃ§in geliÅŸmiÅŸ Ã¶zellik Ã§Ä±karÄ±mÄ± ve analizini iÃ§erir

# Gerekli kÃ¼tÃ¼phaneleri import et
import pandas as pd
import numpy as np
import emoji
import re
from datetime import datetime
from collections import Counter
import os
import joblib
from scipy import stats
import statsmodels.api as sm
from statsmodels.multivariate.manova import MANOVA
import random

# Sklearn kÃ¼tÃ¼phaneleri
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, roc_auc_score
from sklearn.utils.class_weight import compute_class_weight
from statsmodels.stats.multicomp import pairwise_tukeyhsd

# GÃ¶rselleÅŸtirme kÃ¼tÃ¼phaneleri
import matplotlib.pyplot as plt
import seaborn as sns

# SMOTE iÃ§in
from imblearn.over_sampling import SMOTE

# TextBlob ve VADER iÃ§in
from textblob import TextBlob
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer

# GÃ¼venilirlik analizi iÃ§in
from sklearn.preprocessing import StandardScaler

random.seed(42)  # Tekrarlanabilirlik iÃ§in

class SentimentFeatureAnalyzer:
    def __init__(self, data_path='dataset/cleaned_sentiment_dataset.csv'):
        """SÄ±nÄ±f baÅŸlatÄ±cÄ±"""
        self.df = pd.read_csv(data_path)
        print(f"Orijinal veri seti boyutu: {self.df.shape}")
        
        # DuygularÄ± basitleÅŸtir
        self.df['sentiment'] = self.df['Sentiment'].map(
            lambda x: 'positive' if x in ['happy', 'joy', 'excitement', 'love', 'amazing']
            else 'negative' if x in ['sad', 'anger', 'fear', 'disgust', 'disappointed']
            else 'neutral'
        )
        
        # SÄ±nÄ±f daÄŸÄ±lÄ±mÄ±nÄ± gÃ¶ster
        print("\nOrijinal sÄ±nÄ±f daÄŸÄ±lÄ±mÄ±:")
        print(self.df['sentiment'].value_counts())
        
        # Veriyi dengele
        self.balance_dataset()
        
        # Duygu sÃ¶zlÃ¼ÄŸÃ¼ - geniÅŸletilmiÅŸ
        self.sentiment_words = {
            'positive': [
                'happy', 'great', 'awesome', 'excellent', 'good', 'wonderful', 
                'amazing', 'fantastic', 'joy', 'love', 'excited', 'perfect',
                'beautiful', 'brilliant', 'delighted', 'blessed', 'glad'
            ],
            'negative': [
                'sad', 'bad', 'terrible', 'awful', 'horrible', 'disappointed',
                'angry', 'upset', 'hate', 'miserable', 'hurt', 'depressed',
                'worried', 'frustrated', 'unhappy', 'painful', 'worst'
            ]
        }
        
        # Emoji sÃ¶zlÃ¼ÄŸÃ¼ - geniÅŸletilmiÅŸ
        self.emoji_patterns = {
            'positive': r'[ğŸ˜ŠğŸ™‚ğŸ˜„ğŸ˜ƒğŸ˜€ğŸ¥°ğŸ˜ğŸ¤—ğŸ˜ğŸ‘â¤ï¸ğŸ’•]',
            'negative': r'[ğŸ˜¢ğŸ˜­ğŸ˜ğŸ˜”ğŸ˜ŸğŸ˜©ğŸ˜«ğŸ˜–ğŸ˜£ğŸ˜•ğŸ‘ğŸ’”]'
        }
        
        print(f"Veri seti yÃ¼klendi. Boyut: {self.df.shape}")
        
        # Figures klasÃ¶rÃ¼nÃ¼ oluÅŸtur (eÄŸer yoksa)
        if not os.path.exists('figures'):
            os.makedirs('figures')
        
    def balance_dataset(self):
        """GeliÅŸmiÅŸ veri seti dengeleme ve bÃ¼yÃ¼tme"""
        print("\nVeri seti dengeleme ve bÃ¼yÃ¼tme iÅŸlemi baÅŸlÄ±yor...")
        
        # Her sÄ±nÄ±ftan tam 100 Ã¶rnek hedefi
        target_samples = 100
        balanced_dfs = []
        
        for sentiment in ['positive', 'negative', 'neutral']:
            sentiment_df = self.df[self.df['sentiment'] == sentiment].copy()
            current_samples = len(sentiment_df)
            
            if current_samples < target_samples:
                # Daha agresif veri artÄ±rma (Ã¶zellikle negative sÄ±nÄ±fÄ± iÃ§in)
                augmented_texts = []
                augmented_sentiments = []
                
                for _, row in sentiment_df.iterrows():
                    text = row['Text']
                    
                    # 1. Orijinal metin
                    augmented_texts.append(text)
                    augmented_sentiments.append(sentiment)
                    
                    # 2. Emoji varyasyonu
                    emoji_text = self.add_emoji_variations(text, sentiment)
                    augmented_texts.append(emoji_text)
                    augmented_sentiments.append(sentiment)
                    
                    # 3. Noktalama varyasyonu
                    punct_text = self.add_punctuation_variations(text, sentiment)
                    augmented_texts.append(punct_text)
                    augmented_sentiments.append(sentiment)
                    
                    # 4. Kelime sÄ±rasÄ± deÄŸiÅŸimi
                    reordered_text = self.reorder_words(text)
                    augmented_texts.append(reordered_text)
                    augmented_sentiments.append(sentiment)
                    
                    # 5. Emoji + Noktalama kombinasyonu
                    emoji_punct_text = self.add_emoji_variations(
                        self.add_punctuation_variations(text, sentiment),
                        sentiment
                    )
                    augmented_texts.append(emoji_punct_text)
                    augmented_sentiments.append(sentiment)
                    
                    if len(augmented_texts) >= target_samples:
                        break
                
                # Tam olarak target_samples kadar Ã¶rnek al
                augmented_df = pd.DataFrame({
                    'Text': augmented_texts[:target_samples],
                    'sentiment': augmented_sentiments[:target_samples]
                })
                balanced_dfs.append(augmented_df)
            else:
                # Rastgele Ã¶rnekleme
                balanced_df = sentiment_df.sample(n=target_samples, random_state=42)
                balanced_dfs.append(balanced_df)
        
        # Dengeli veri setini oluÅŸtur
        self.df = pd.concat(balanced_dfs, ignore_index=True)
        
        print("\nDengelenmiÅŸ veri seti daÄŸÄ±lÄ±mÄ±:")
        print(self.df['sentiment'].value_counts())
        
        return self.df

    def add_emoji_variations(self, text, sentiment):
        """Duygu durumuna gÃ¶re geliÅŸmiÅŸ emoji varyasyonlarÄ± ekle"""
        emoji_map = {
            'positive': ['ğŸ˜Š', 'ğŸ‘', 'â¤ï¸', 'ğŸ‰', 'ğŸ˜„', 'âœ¨', 'ğŸ’ª', 'ğŸŒŸ'],
            'negative': ['ğŸ˜¢', 'ğŸ‘', 'ğŸ˜ ', 'ğŸ˜”', 'ğŸ’”', 'ğŸ˜', 'ğŸ˜£', 'ğŸ˜«'],
            'neutral': ['ğŸ¤”', 'ğŸ˜', 'ğŸ’­', 'ğŸ“', 'ğŸ’¡', 'ğŸ”', 'ğŸ“Œ', 'ğŸ’¬']
        }
        
        # Mevcut metne 1-3 emoji ekle
        emojis = random.sample(emoji_map[sentiment], random.randint(1, 3))
        
        # Emojileri metnin baÅŸÄ±na veya sonuna rastgele ekle
        if random.choice([True, False]):
            return ' '.join(emojis) + ' ' + text
        return text + ' ' + ' '.join(emojis)

    def add_punctuation_variations(self, text, sentiment):
        """Duygu durumuna gÃ¶re geliÅŸmiÅŸ noktalama varyasyonlarÄ± ekle"""
        punct_map = {
            'positive': ['!', '!!', '...!', '! :)', '!!! ğŸ‰', '~'],
            'negative': ['...', '?!', '!!?', '... :(', '?!?', '...?'],
            'neutral': ['.', '...', '?', '...?', '. -', '...']
        }
        
        # Rastgele 1-2 noktalama iÅŸareti ekle
        puncts = random.sample(punct_map[sentiment], random.randint(1, 2))
        return text + ''.join(puncts)

    def reorder_words(self, text):
        """Kelime sÄ±rasÄ±nÄ± deÄŸiÅŸtir (anlamÄ± koruyarak)"""
        words = text.split()
        if len(words) <= 3:  # Ã‡ok kÄ±sa metinlerde deÄŸiÅŸiklik yapma
            return text
        
        # CÃ¼mlenin ortasÄ±ndaki kelimelerin sÄ±rasÄ±nÄ± deÄŸiÅŸtir
        mid_start = len(words) // 3
        mid_end = len(words) - len(words) // 3
        middle_words = words[mid_start:mid_end]
        random.shuffle(middle_words)
        
        return ' '.join(words[:mid_start] + middle_words + words[mid_end:])
    
    def extract_features(self, text):
        """GeliÅŸtirilmiÅŸ Ã¶zellik Ã§Ä±karÄ±mÄ±"""
        features = {}
        text = str(text).lower()
        
        # Temel metin Ã¶zellikleri
        features['text_length'] = len(text)
        features['word_count'] = len(text.split())
        features['avg_word_length'] = np.mean([len(w) for w in text.split()])
        
        # Duygu kelimeleri sayÄ±sÄ± - aÄŸÄ±rlÄ±klÄ±
        features['positive_words'] = sum(3 for word in text.split() 
                                     if word in self.sentiment_words['positive'])
        features['negative_words'] = sum(3 for word in text.split() 
                                     if word in self.sentiment_words['negative'])
        
        # Emoji sayÄ±sÄ± - aÄŸÄ±rlÄ±klÄ±
        features['positive_emoji'] = len(re.findall(self.emoji_patterns['positive'], text)) * 2
        features['negative_emoji'] = len(re.findall(self.emoji_patterns['negative'], text)) * 2
        
        # Noktalama iÅŸaretleri - aÄŸÄ±rlÄ±klÄ±
        features['exclamation_count'] = text.count('!') * 1.5
        features['question_count'] = text.count('?')
        
        # BÃ¼yÃ¼k harf kullanÄ±mÄ±
        features['caps_ratio'] = sum(1 for c in text if c.isupper()) / len(text) if len(text) > 0 else 0
        
        return features
    
    def train_model(self):
        """Model eÄŸitimi"""
        print("\nModel eÄŸitimi baÅŸlÄ±yor...")
        
        # Ã–zellik Ã§Ä±karÄ±mÄ±
        features_df = pd.DataFrame([
            self.extract_features(text) for text in self.df['Text']
        ])
        
        # TF-IDF Ã¶zellikleri
        self.tfidf = TfidfVectorizer(
            max_features=100,
            stop_words='english'
        )
        tfidf_features = self.tfidf.fit_transform(self.df['Text'])
        
        # Ã–zellikleri birleÅŸtir
        X = np.hstack([
            features_df.values,
            tfidf_features.toarray()
        ])
        
        # Etiketleri hazÄ±rla - sadece 3 sÄ±nÄ±f
        self.df['simple_sentiment'] = self.df['Sentiment'].map(
            lambda x: 'positive' if x in self.sentiment_words['positive']
            else 'negative' if x in self.sentiment_words['negative']
            else 'neutral'
        )
        
        self.label_encoder = LabelEncoder()
        y = self.label_encoder.fit_transform(self.df['simple_sentiment'])
        
        # Veriyi bÃ¶l - stratify olmadan
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42
        )
        
        # Model
        self.model = RandomForestClassifier(
            n_estimators=200,
            max_depth=10,
            class_weight='balanced',
            random_state=42,
            n_jobs=-1
        )
        
        # EÄŸitim
        print("Model eÄŸitiliyor...")
        self.model.fit(X_train, y_train)
        
        # DeÄŸerlendirme
        y_pred = self.model.predict(X_test)
        print(f"\nModel DoÄŸruluÄŸu: {accuracy_score(y_test, y_pred):.2%}")
        print("\nSÄ±nÄ±f BazÄ±nda Performans:")
        print(classification_report(y_test, y_pred))
        
        return self.model
    
    def predict_emotion(self, text):
        """Duygu tahmini"""
        try:
            # Ã–zellik Ã§Ä±karÄ±mÄ±
            features = pd.DataFrame([self.extract_features(text)])
            tfidf_features = self.tfidf.transform([text])
            
            # Ã–zellikleri birleÅŸtir
            X = np.hstack([
                features.values,
                tfidf_features.toarray()
            ])
            
            # Tahmin
            probs = self.model.predict_proba(X)[0]
            classes = self.label_encoder.classes_
            
            # SonuÃ§larÄ± hazÄ±rla
            results = dict(zip(classes, [round(p * 100, 2) for p in probs]))
            prediction = max(results.items(), key=lambda x: x[1])[0]
            
            return {
                'prediction': prediction,
                'confidence': results[prediction],
                'probabilities': results
            }
            
        except Exception as e:
            print(f"Hata: {str(e)}")
            return None
    
    def analyze_sentiment(self):
        """VADER ile duygu analizi"""
        analyzer = SentimentIntensityAnalyzer()
        
        # VADER skorlarÄ±nÄ± hesapla
        self.df['vader_scores'] = self.df['Text'].apply(
            lambda x: analyzer.polarity_scores(str(x))
        )
        
        # SkorlarÄ± ayrÄ± kolonlara ayÄ±r
        self.df['vader_neg'] = self.df['vader_scores'].apply(lambda x: x['neg'])
        self.df['vader_neu'] = self.df['vader_scores'].apply(lambda x: x['neu'])
        self.df['vader_pos'] = self.df['vader_scores'].apply(lambda x: x['pos'])
        self.df['vader_compound'] = self.df['vader_scores'].apply(lambda x: x['compound'])

    def perform_hypothesis_test(self):
        """Hipotez testleri"""
        print("\nHipotez Testleri:")
        print("=" * 50)
        
        # H0: VADER skorlarÄ± ile etiketler arasÄ±nda iliÅŸki yoktur
        # H1: VADER skorlarÄ± ile etiketler arasÄ±nda anlamlÄ± bir iliÅŸki vardÄ±r
        
        # Her duygu iÃ§in VADER compound skorlarÄ±nÄ± grupla
        pos_scores = self.df[self.df['sentiment'] == 'positive']['vader_compound']
        neg_scores = self.df[self.df['sentiment'] == 'negative']['vader_compound']
        neu_scores = self.df[self.df['sentiment'] == 'neutral']['vader_compound']
        
        # Kruskal-Wallis H-test
        h_stat, p_value = stats.kruskal(pos_scores, neg_scores, neu_scores)
        
        print("\n1. Kruskal-Wallis H-test:")
        print(f"H-istatistiÄŸi: {h_stat:.4f}")
        print(f"p-deÄŸeri: {p_value:.4f}")
        print(f"SonuÃ§: {'H0 reddedilir' if p_value < 0.05 else 'H0 reddedilemez'}")
        
        # Mann-Whitney U test (ikili karÅŸÄ±laÅŸtÄ±rmalar)
        print("\n2. Mann-Whitney U testleri:")
        
        # Positive vs Negative
        u_stat, p_value = stats.mannwhitneyu(pos_scores, neg_scores, alternative='two-sided')
        print("\nPositive vs Negative:")
        print(f"U-istatistiÄŸi: {u_stat:.4f}")
        print(f"p-deÄŸeri: {p_value:.4f}")
        print(f"SonuÃ§: {'AnlamlÄ± fark var' if p_value < 0.05 else 'AnlamlÄ± fark yok'}")
        
        # Positive vs Neutral
        u_stat, p_value = stats.mannwhitneyu(pos_scores, neu_scores, alternative='two-sided')
        print("\nPositive vs Neutral:")
        print(f"U-istatistiÄŸi: {u_stat:.4f}")
        print(f"p-deÄŸeri: {p_value:.4f}")
        print(f"SonuÃ§: {'AnlamlÄ± fark var' if p_value < 0.05 else 'AnlamlÄ± fark yok'}")
        
        # Negative vs Neutral
        u_stat, p_value = stats.mannwhitneyu(neg_scores, neu_scores, alternative='two-sided')
        print("\nNegative vs Neutral:")
        print(f"U-istatistiÄŸi: {u_stat:.4f}")
        print(f"p-deÄŸeri: {p_value:.4f}")
        print(f"SonuÃ§: {'AnlamlÄ± fark var' if p_value < 0.05 else 'AnlamlÄ± fark yok'}")
        
        # GÃ¶rselleÅŸtirme
        self.visualize_results()
        
    def visualize_results(self):
        """SonuÃ§larÄ± gÃ¶rselleÅŸtir"""
        plt.figure(figsize=(10, 6))
        sns.boxplot(x='sentiment', y='vader_compound', data=self.df)
        plt.title('VADER Compound Scores by Sentiment Class')
        plt.show()

    def define_research_questions(self):
        """
        AraÅŸtÄ±rma sorularÄ±nÄ± ve hipotezleri tanÄ±mla
        """
        print("\nAraÅŸtÄ±rma SorularÄ± ve Hipotezler:")
        print("=" * 50)
        
        research_questions = {
            "RQ1": "Sosyal medya metinlerindeki duygu polaritesi ile kullanÄ±cÄ± etkileÅŸimi arasÄ±nda bir iliÅŸki var mÄ±?",
            "RQ2": "Emoji kullanÄ±mÄ± ile metnin duygu skoru arasÄ±nda anlamlÄ± bir iliÅŸki var mÄ±?",
            "RQ3": "Metin uzunluÄŸu ile duygu yoÄŸunluÄŸu arasÄ±nda bir korelasyon var mÄ±?"
        }
        
        hypotheses = {
            "H1": {
                "null": "Duygu polaritesi ile etkileÅŸim sayÄ±sÄ± arasÄ±nda anlamlÄ± bir iliÅŸki yoktur",
                "alternative": "Duygu polaritesi ile etkileÅŸim sayÄ±sÄ± arasÄ±nda anlamlÄ± bir iliÅŸki vardÄ±r"
            },
            "H2": {
                "null": "Emoji kullanÄ±mÄ± ile VADER duygu skoru arasÄ±nda anlamlÄ± bir iliÅŸki yoktur",
                "alternative": "Emoji kullanÄ±mÄ± ile VADER duygu skoru arasÄ±nda anlamlÄ± bir iliÅŸki vardÄ±r"
            },
            "H3": {
                "null": "Metin uzunluÄŸu ile duygu yoÄŸunluÄŸu arasÄ±nda anlamlÄ± bir korelasyon yoktur",
                "alternative": "Metin uzunluÄŸu ile duygu yoÄŸunluÄŸu arasÄ±nda anlamlÄ± bir korelasyon vardÄ±r"
            }
        }
        
        for rq, question in research_questions.items():
            print(f"\n{rq}: {question}")
        
        print("\nHipotezler:")
        for h, hyp in hypotheses.items():
            print(f"\n{h}:")
            print(f"H0: {hyp['null']}")
            print(f"H1: {hyp['alternative']}")

    def perform_extended_analysis(self):
        """
        GeniÅŸletilmiÅŸ veri analizi
        """
        # Metin Ã¶zellikleri analizi
        self.df['text_length'] = self.df['Text'].str.len()
        self.df['word_count'] = self.df['Text'].str.split().str.len()
        self.df['emoji_count'] = self.df['Text'].apply(lambda x: len(re.findall(r'[^\w\s,]', str(x))))
        
        # Korelasyon analizi
        correlation_matrix = self.df[[
            'text_length', 'word_count', 'emoji_count', 
            'vader_compound', 'vader_pos', 'vader_neg'
        ]].corr()
        
        # TanÄ±mlayÄ±cÄ± istatistikler
        descriptive_stats = self.df.describe()
        
        return correlation_matrix, descriptive_stats

    def perform_advanced_statistical_tests(self):
        """
        GeliÅŸmiÅŸ istatistiksel testler
        """
        # Normallik testi
        normality_test = stats.shapiro(self.df['vader_compound'])
        
        # Spearman korelasyonu
        correlation_text_sentiment = stats.spearmanr(
            self.df['text_length'], 
            self.df['vader_compound']
        )
        
        # ANOVA testi
        groups = [
            self.df[self.df['sentiment'] == 'positive']['vader_compound'],
            self.df[self.df['sentiment'] == 'negative']['vader_compound'],
            self.df[self.df['sentiment'] == 'neutral']['vader_compound']
        ]
        f_stat, anova_p = stats.f_oneway(*groups)
        
        return {
            'normality': normality_test,
            'correlation': correlation_text_sentiment,
            'anova': (f_stat, anova_p)
        }

    def perform_comprehensive_analysis(self):
        """
        KapsamlÄ± istatistiksel analiz
        """
        print("\nKapsamlÄ± Ä°statistiksel Analiz")
        print("=" * 50)

        # 1. Temel Ã–zellik HesaplamalarÄ±
        self.df['text_length'] = self.df['Text'].str.len()
        self.df['word_count'] = self.df['Text'].str.split().str.len()
        self.df['emoji_count'] = self.df['Text'].apply(lambda x: len(re.findall(r'[^\w\s,]', str(x))))
        self.df['avg_word_length'] = self.df['Text'].apply(lambda x: np.mean([len(word) for word in str(x).split()]))

        # 2. Korelasyon Analizleri
        print("\n1. Spearman Korelasyon Analizleri:")
        features = ['text_length', 'word_count', 'emoji_count', 'avg_word_length']
        sentiment_scores = ['vader_compound', 'vader_pos', 'vader_neg', 'vader_neu']
        
        for feature in features:
            for score in sentiment_scores:
                correlation, p_value = stats.spearmanr(self.df[feature], self.df[score])
                if p_value < 0.05:
                    print(f"{feature} vs {score}:")
                    print(f"Correlation: {correlation:.3f}, p-value: {p_value:.4f}")

        # 3. Effect Size HesaplamalarÄ±
        print("\n2. Effect Size Analizi:")
        def cohen_d(x, y):
            nx = len(x)
            ny = len(y)
            dof = nx + ny - 2
            return (np.mean(x) - np.mean(y)) / np.sqrt(((nx-1)*np.std(x, ddof=1) ** 2 + (ny-1)*np.std(y, ddof=1) ** 2) / dof)

        # Duygu sÄ±nÄ±flarÄ± arasÄ± effect size
        pos_scores = self.df[self.df['sentiment'] == 'positive']['vader_compound']
        neg_scores = self.df[self.df['sentiment'] == 'negative']['vader_compound']
        neu_scores = self.df[self.df['sentiment'] == 'neutral']['vader_compound']

        d_pos_neg = cohen_d(pos_scores, neg_scores)
        d_pos_neu = cohen_d(pos_scores, neu_scores)
        d_neg_neu = cohen_d(neg_scores, neu_scores)

        print(f"Cohen's d (positive vs negative): {d_pos_neg:.3f}")
        print(f"Cohen's d (positive vs neutral): {d_pos_neu:.3f}")
        print(f"Cohen's d (negative vs neutral): {d_neg_neu:.3f}")

        # 4. Regresyon Analizi
        print("\n3. Basit DoÄŸrusal Regresyon Analizleri:")
        from sklearn.linear_model import LinearRegression
        from sklearn.metrics import r2_score

        for feature in features:
            X = self.df[feature].values.reshape(-1, 1)
            y = self.df['vader_compound'].values
            
            model = LinearRegression()
            model.fit(X, y)
            y_pred = model.predict(X)
            r2 = r2_score(y, y_pred)
            
            print(f"\n{feature} -> vader_compound:")
            print(f"RÂ²: {r2:.3f}")
            print(f"Coefficient: {model.coef_[0]:.3f}")
            print(f"Intercept: {model.intercept_:.3f}")

        # 5. GÃ¼Ã§ Analizi
        print("\n4. GÃ¼Ã§ Analizi:")
        from statsmodels.stats.power import TTestPower
        
        effect_sizes = [0.2, 0.5, 0.8]  # small, medium, large
        n = len(self.df)
        
        print("\nMevcut Ã¶rneklem bÃ¼yÃ¼klÃ¼ÄŸÃ¼ iÃ§in gÃ¼Ã§ deÄŸerleri:")
        for effect in effect_sizes:
            power = TTestPower().power(effect_size=effect, nobs=n, alpha=0.05)
            print(f"Effect size {effect}: Power = {power:.3f}")

        # 6. Normallik Testleri
        print("\n5. Normallik Testleri (Shapiro-Wilk):")
        for score in sentiment_scores:
            stat, p_value = stats.shapiro(self.df[score])
            print(f"{score}:")
            print(f"Statistic: {stat:.3f}, p-value: {p_value:.4f}")
            print(f"Normal daÄŸÄ±lÄ±m: {'HayÄ±r' if p_value < 0.05 else 'Evet'}")

    def perform_advanced_analysis(self):
        """
        GeliÅŸmiÅŸ analizler
        """
        results = {}
        
        # 1. Bootstrap Analysis
        compound_scores = self.df['vader_compound'].values
        bootstrap_means = []
        for _ in range(1000):
            sample = np.random.choice(compound_scores, size=len(compound_scores), replace=True)
            bootstrap_means.append(np.mean(sample))
        results['bootstrap_mean_ci'] = np.percentile(bootstrap_means, [2.5, 97.5])

        # 2. Cross Validation
        X = self.df[['text_length', 'word_count', 'emoji_count']].values
        y = self.df['vader_compound'].values
        cv_scores = cross_val_score(RandomForestRegressor(random_state=42), X, y, cv=5)
        results['cross_validation'] = (cv_scores.mean(), cv_scores.std())

        # 3. Chi-square test
        contingency = pd.crosstab(
            self.df['sentiment'],
            pd.qcut(self.df['vader_compound'], q=3, labels=['low', 'medium', 'high'])
        )
        results['chi_square'] = stats.chi2_contingency(contingency)[:2]

        # 4. MANOVA
        features = ['vader_compound', 'text_length', 'emoji_count']
        X = self.df[features]
        manova = MANOVA.from_formula('X ~ sentiment', data=self.df)
        results['manova'] = manova.mv_test()

        # 5. Post-hoc
        tukey = pairwise_tukeyhsd(
            self.df['vader_compound'],
            self.df['sentiment'],
            alpha=0.05
        )
        results['post_hoc'] = tukey

        return results

    def reliability_analysis(self):
        """
        GÃ¼venilirlik analizi
        """
        # Cronbach's alpha iÃ§in Ã¶zellikler
        features = ['vader_compound', 'text_length', 'emoji_count']
        X = StandardScaler().fit_transform(self.df[features])
        
        # Korelasyon matrisi
        corr_matrix = np.corrcoef(X.T)
        n = len(features)
        cronbach_alpha = (n / (n-1)) * (1 - np.trace(corr_matrix) / np.sum(corr_matrix))

        # Test-retest iÃ§in split-half reliability
        n_samples = len(self.df)
        first_half = X[:n_samples//2]
        second_half = X[n_samples//2:]
        reliability = np.corrcoef(
            np.mean(first_half, axis=1),
            np.mean(second_half, axis=1)
        )[0,1]

        return {
            'cronbach_alpha': cronbach_alpha,
            'test_retest': reliability
        }

    def create_advanced_visualizations(self):
        """
        GeliÅŸmiÅŸ gÃ¶rselleÅŸtirmeler ve kaydetme
        """
        # 1. Duygu DaÄŸÄ±lÄ±mÄ±
        plt.figure(figsize=(10, 6))
        sns.violinplot(x='sentiment', y='vader_compound', data=self.df)
        plt.title('Duygu SkorlarÄ± DaÄŸÄ±lÄ±mÄ±')
        plt.savefig('figures/sentiment_distribution.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        # 2. Regresyon Analizi
        plt.figure(figsize=(10, 6))
        sns.regplot(x='text_length', y='vader_compound', data=self.df)
        plt.title('Metin UzunluÄŸu vs Duygu Skoru')
        plt.savefig('figures/regression_analysis.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        # 3. Korelasyon Matrisi
        plt.figure(figsize=(8, 6))
        correlation_matrix = self.df[['vader_compound', 'text_length', 
                                    'emoji_count', 'word_count']].corr()
        sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
        plt.title('Korelasyon Matrisi')
        plt.savefig('figures/correlation_matrix.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        # 4. Box Plot - Duygu SÄ±nÄ±flarÄ±
        plt.figure(figsize=(10, 6))
        sns.boxplot(x='sentiment', y='vader_compound', data=self.df)
        plt.title('Duygu SÄ±nÄ±flarÄ±na GÃ¶re VADER SkorlarÄ±')
        plt.savefig('figures/sentiment_boxplot.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        print("\nGÃ¶rselleÅŸtirmeler 'figures' klasÃ¶rÃ¼ne kaydedildi:")
        print("1. sentiment_distribution.png")
        print("2. regression_analysis.png")
        print("3. correlation_matrix.png")
        print("4. sentiment_boxplot.png")

    def improve_reliability(self):
        """GÃ¼venilirlik metriklerini iyileÅŸtirme"""
        
        # 1. Feature Selection
        def select_important_features(self):
            """Ã–nemli Ã¶zellikleri seÃ§"""
            # Korelasyon bazlÄ± feature selection
            features_df = pd.DataFrame([
                self.extract_features(text) for text in self.df['Text']
            ])
            
            # VADER skorlarÄ± ile korelasyonu yÃ¼ksek Ã¶zellikleri seÃ§
            correlations = {}
            for column in features_df.columns:
                corr = stats.spearmanr(features_df[column], self.df['vader_compound'])[0]
                correlations[column] = abs(corr)
            
            # En yÃ¼ksek korelasyona sahip Ã¶zellikleri seÃ§
            important_features = [k for k, v in sorted(correlations.items(), 
                                                     key=lambda x: x[1], 
                                                     reverse=True)[:5]]
            return important_features
        
        # 2. Veri Kalitesi KontrolÃ¼
        def check_data_quality(self):
            """Veri kalitesi kontrolÃ¼ ve temizleme"""
            # AykÄ±rÄ± deÄŸerleri tespit et
            z_scores = stats.zscore(self.df[['vader_compound', 'vader_pos', 'vader_neg', 'vader_neu']])
            abs_z_scores = np.abs(z_scores)
            outlier_mask = (abs_z_scores < 3).all(axis=1)
            
            # AykÄ±rÄ± deÄŸerleri filtrele
            self.df = self.df[outlier_mask]
            
            # TutarsÄ±z etiketleri kontrol et
            vader_pred = np.where(self.df['vader_compound'] > 0.05, 'positive',
                                np.where(self.df['vader_compound'] < -0.05, 'negative', 'neutral'))
            
            # TutarlÄ± Ã¶rnekleri seÃ§
            consistency_mask = vader_pred == self.df['sentiment']
            self.df = self.df[consistency_mask]
            
            return self.df
        
        # 3. Feature Engineering
        def engineer_new_features(self):
            """Yeni Ã¶zellikler oluÅŸtur"""
            # Emoji yoÄŸunluÄŸu
            self.df['emoji_density'] = self.df['Text'].apply(
                lambda x: len(re.findall(r'[^\w\s,]', str(x))) / len(str(x))
            )
            
            # Duygu kelimesi yoÄŸunluÄŸu
            self.df['sentiment_word_density'] = self.df['Text'].apply(
                lambda x: sum(1 for word in str(x).split() 
                            if word in self.sentiment_words['positive'] 
                            or word in self.sentiment_words['negative']) / len(str(x).split())
            )
            
            # Noktalama iÅŸareti yoÄŸunluÄŸu
            self.df['punctuation_density'] = self.df['Text'].apply(
                lambda x: len(re.findall(r'[!?.]', str(x))) / len(str(x))
            )
            
            return self.df

        # Ana fonksiyon
        print("\nGÃ¼venilirlik iyileÅŸtirme sÃ¼reci baÅŸlÄ±yor...")
        
        # 1. Ã–nemli Ã¶zellikleri seÃ§
        important_features = select_important_features(self)
        print(f"\nSeÃ§ilen Ã¶nemli Ã¶zellikler: {important_features}")
        
        # 2. Veri kalitesini kontrol et
        original_size = len(self.df)
        self.df = check_data_quality(self)
        print(f"\nVeri kalitesi kontrolÃ¼ sonrasÄ± Ã¶rneklem sayÄ±sÄ±: {len(self.df)}")
        print(f"Ã‡Ä±karÄ±lan Ã¶rnek sayÄ±sÄ±: {original_size - len(self.df)}")
        
        # 3. Yeni Ã¶zellikler ekle
        self.df = engineer_new_features(self)
        print("\nYeni Ã¶zellikler eklendi")
        
        # 4. GÃ¼venilirlik metriklerini tekrar hesapla
        cronbach_alpha = self.calculate_cronbach_alpha()
        test_retest = self.calculate_test_retest_reliability()
        
        print("\nGÃ¼ncellenmiÅŸ gÃ¼venilirlik metrikleri:")
        print(f"Cronbach's alpha: {cronbach_alpha:.3f}")
        print(f"Test-retest gÃ¼venilirliÄŸi: {test_retest:.3f}")
        
        return self.df

    def improve_model_performance(self):
        """Model performansÄ±nÄ± iyileÅŸtirme"""
        
        # 1. GeliÅŸmiÅŸ Feature Engineering
        def create_advanced_features(self):
            features = {}
            
            # TF-IDF Ã¶zellikleri
            tfidf = TfidfVectorizer(max_features=100)
            tfidf_features = tfidf.fit_transform(self.df['Text'])
            
            # Duygu yoÄŸunluÄŸu Ã¶zellikleri
            features['sentiment_intensity'] = self.df['vader_compound'].abs()
            features['emotion_contrast'] = self.df['vader_pos'] - self.df['vader_neg']
            
            # Metin karmaÅŸÄ±klÄ±ÄŸÄ± Ã¶zellikleri
            features['text_complexity'] = self.df['Text'].apply(
                lambda x: len(set(x.split())) / len(x.split()) if len(x.split()) > 0 else 0
            )
            
            return pd.DataFrame(features), tfidf_features
        
        # 2. Model Optimizasyonu
        def optimize_model(self, X, y):
            # Grid Search iÃ§in parametreler
            param_grid = {
                'n_estimators': [100, 200, 300],
                'max_depth': [5, 10, 15],
                'min_samples_split': [2, 5, 10],
                'min_samples_leaf': [1, 2, 4]
            }
            
            # Grid Search
            rf = RandomForestClassifier(random_state=42)
            from sklearn.model_selection import GridSearchCV
            grid_search = GridSearchCV(
                estimator=rf,
                param_grid=param_grid,
                cv=5,
                n_jobs=-1,
                scoring='accuracy'
            )
            
            grid_search.fit(X, y)
            return grid_search.best_estimator_, grid_search.best_params_
        
        # 3. Cross-validation iyileÅŸtirmesi
        def improved_cross_validation(self, X, y):
            # Stratified K-Fold
            from sklearn.model_selection import StratifiedKFold
            skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
            
            # SonuÃ§larÄ± sakla
            cv_scores = []
            
            for train_idx, test_idx in skf.split(X, y):
                X_train, X_test = X[train_idx], X[test_idx]
                y_train, y_test = y[train_idx], y[test_idx]
                
                # SMOTE uygula
                smote = SMOTE(random_state=42)
                X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)
                
                # Model eÄŸitimi
                model = self.model.fit(X_train_balanced, y_train_balanced)
                
                # Tahmin
                y_pred = model.predict(X_test)
                
                # Skor hesapla
                cv_scores.append(accuracy_score(y_test, y_pred))
            
            return np.mean(cv_scores), np.std(cv_scores)
        
        print("\nModel performansÄ± iyileÅŸtirme sÃ¼reci baÅŸlÄ±yor...")
        
        # 1. GeliÅŸmiÅŸ Ã¶zellikler oluÅŸtur
        advanced_features, tfidf_features = create_advanced_features(self)
        X = np.hstack([advanced_features, tfidf_features.toarray()])
        y = self.df['sentiment']
        
        # 2. Model optimizasyonu
        best_model, best_params = optimize_model(self, X, y)
        print(f"\nEn iyi model parametreleri: {best_params}")
        
        # 3. Ä°yileÅŸtirilmiÅŸ cross-validation
        cv_mean, cv_std = improved_cross_validation(self, X, y)
        print(f"\nÄ°yileÅŸtirilmiÅŸ Cross-validation sonuÃ§larÄ±:")
        print(f"Ortalama accuracy: {cv_mean:.3f} (Â±{cv_std:.3f})")
        
        # 4. Final model performansÄ±
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
        
        final_model = best_model.fit(X_train, y_train)
        y_pred = final_model.predict(X_test)
        
        print("\nFinal Model PerformansÄ±:")
        print(f"Accuracy: {accuracy_score(y_test, y_pred):.3f}")
        print("\nSÄ±nÄ±flandÄ±rma Raporu:")
        print(classification_report(y_test, y_pred))
        
        return final_model

    def perform_robustness_analysis(self):
        """SaÄŸlamlÄ±k testleri ve duyarlÄ±lÄ±k analizi"""
        
        print("\nSaÄŸlamlÄ±k ve DuyarlÄ±lÄ±k Analizi BaÅŸlÄ±yor...")
        results = {}
        
        # 1. Bootstrap Stability Analysis
        def bootstrap_stability(self, n_iterations=1000):
            stability_scores = []
            for _ in range(n_iterations):
                # Bootstrap Ã¶rneklemi
                bootstrap_idx = np.random.choice(len(self.df), size=len(self.df), replace=True)
                bootstrap_sample = self.df.iloc[bootstrap_idx]
                
                # Temel metrikleri hesapla
                corr = stats.spearmanr(
                    bootstrap_sample['text_length'],
                    bootstrap_sample['vader_compound']
                )[0]
                stability_scores.append(corr)
            
            return np.mean(stability_scores), np.std(stability_scores)
        
        # 2. Sensitivity Analysis
        def sensitivity_analysis(self):
            sensitivities = {}
            
            # Her bir Ã¶zellik iÃ§in etki analizi
            features = ['text_length', 'word_count', 'emoji_count', 'avg_word_length']
            for feature in features:
                # Ã–zelliÄŸi %10 artÄ±r/azalt
                delta = self.df[feature].std() * 0.1
                
                # ArtÄ±rÄ±lmÄ±ÅŸ deÄŸerler
                increased = self.df[feature] + delta
                corr_increased = stats.spearmanr(increased, self.df['vader_compound'])[0]
                
                # AzaltÄ±lmÄ±ÅŸ deÄŸerler
                decreased = self.df[feature] - delta
                corr_decreased = stats.spearmanr(decreased, self.df['vader_compound'])[0]
                
                # DuyarlÄ±lÄ±k skoru
                sensitivity = abs(corr_increased - corr_decreased) / (2 * delta)
                sensitivities[feature] = sensitivity
            
            return sensitivities
        
        # 3. Cross-Method Validation
        def cross_method_validation(self):
            # FarklÄ± duygu analizi yÃ¶ntemlerini karÅŸÄ±laÅŸtÄ±r
            methods = {
                'vader': self.df['vader_compound'],
                'textblob': self.df['Text'].apply(lambda x: TextBlob(str(x)).sentiment.polarity),
                'custom': self.df['Text'].apply(self.custom_sentiment_score)
            }
            
            # YÃ¶ntemler arasÄ± korelasyon
            correlations = {}
            for m1 in methods:
                for m2 in methods:
                    if m1 < m2:  # TekrarÄ± Ã¶nle
                        corr = stats.spearmanr(methods[m1], methods[m2])[0]
                        correlations[f'{m1}_vs_{m2}'] = corr
            
            return correlations
        
        # 4. Error Analysis
        def error_analysis(self):
            errors = {
                'false_positives': [],
                'false_negatives': [],
                'misclassified_neutral': []
            }
            
            # VADER tahminleri
            vader_pred = np.where(self.df['vader_compound'] > 0.05, 'positive',
                                np.where(self.df['vader_compound'] < -0.05, 'negative', 'neutral'))
            
            # Hata analizi
            for idx, (true, pred) in enumerate(zip(self.df['sentiment'], vader_pred)):
                if true != pred:
                    text = self.df.iloc[idx]['Text']
                    if true == 'positive' and pred != 'positive':
                        errors['false_negatives'].append(text)
                    elif true == 'negative' and pred != 'negative':
                        errors['false_positives'].append(text)
                    elif true == 'neutral':
                        errors['misclassified_neutral'].append(text)
            
            return errors
        
        # Analizleri Ã§alÄ±ÅŸtÄ±r
        print("\n1. Bootstrap Stability Analysis")
        stability_mean, stability_std = bootstrap_stability(self)
        print(f"Stabilite skoru: {stability_mean:.3f} (Â±{stability_std:.3f})")
        
        print("\n2. Sensitivity Analysis")
        sensitivities = sensitivity_analysis(self)
        for feature, sensitivity in sensitivities.items():
            print(f"{feature}: {sensitivity:.3f}")
        
        print("\n3. Cross-Method Validation")
        method_correlations = cross_method_validation(self)
        for comparison, corr in method_correlations.items():
            print(f"{comparison}: {corr:.3f}")
        
        print("\n4. Error Analysis")
        errors = error_analysis(self)
        for error_type, examples in errors.items():
            print(f"\n{error_type.replace('_', ' ').title()}:")
            print(f"Toplam hata sayÄ±sÄ±: {len(examples)}")
            if examples:
                print("Ã–rnek hatalar:")
                for ex in examples[:3]:  # Ä°lk 3 Ã¶rnek
                    print(f"- {ex}")
        
        return {
            'stability': (stability_mean, stability_std),
            'sensitivities': sensitivities,
            'method_correlations': method_correlations,
            'errors': errors
        }

def main():
    analyzer = SentimentFeatureAnalyzer()
    analyzer.analyze_sentiment()
    
    print("\n=== 1. Temel Analizler ===")
    analyzer.perform_comprehensive_analysis()
    
    print("\n=== 2. Ä°leri DÃ¼zey Analizler ===")
    advanced_results = analyzer.perform_advanced_analysis()
    
    # Bootstrap sonuÃ§larÄ±
    print("\nBootstrap Analizi:")
    if advanced_results and 'bootstrap_mean_ci' in advanced_results:
        bootstrap_ci = advanced_results['bootstrap_mean_ci']
    else:
        bootstrap_ci = [0, 0]  # VarsayÄ±lan deÄŸerler
    print(f"GÃ¼ven AralÄ±ÄŸÄ± (95%): [{bootstrap_ci[0]:.3f}, {bootstrap_ci[1]:.3f}]")
    
    # Cross-validation sonuÃ§larÄ±
    print("\nCross-Validation SonuÃ§larÄ±:")
    if advanced_results and 'cross_validation' in advanced_results:
        cv_mean, cv_std = advanced_results['cross_validation']
    else:
        cv_mean, cv_std = 0, 0  # VarsayÄ±lan deÄŸerler
    print(f"Ortalama RÂ²: {cv_mean:.3f} (Â±{cv_std:.3f})")
    
    # Chi-square test sonuÃ§larÄ±
    print("\nChi-square Test SonuÃ§larÄ±:")
    if advanced_results and 'chi_square' in advanced_results:
        chi2, p_value = advanced_results['chi_square']
    else:
        chi2, p_value = 0, 0  # VarsayÄ±lan deÄŸerler
    print(f"Chi-square: {chi2:.3f}")
    print(f"p-deÄŸeri: {p_value:.4f}")
    
    # MANOVA sonuÃ§larÄ±
    print("\nMANOVA Test SonuÃ§larÄ±:")
    if advanced_results and 'manova' in advanced_results:
        manova_results = advanced_results['manova']
    else:
        manova_results = None  # VarsayÄ±lan deÄŸerler
    print(manova_results)
    
    # Post-hoc analiz sonuÃ§larÄ±
    print("\nPost-hoc Analiz (Tukey HSD):")
    if advanced_results and 'post_hoc' in advanced_results:
        post_hoc_results = advanced_results['post_hoc']
    else:
        post_hoc_results = None  # VarsayÄ±lan deÄŸerler
    print(post_hoc_results)
    
    print("\n=== 3. GÃ¼venilirlik Analizi ===")
    reliability = analyzer.reliability_analysis()
    print(f"Cronbach's alpha: {reliability['cronbach_alpha']:.3f}")
    print(f"Test-retest gÃ¼venilirliÄŸi: {reliability['test_retest']:.3f}")
    
    print("\n=== 4. GÃ¶rselleÅŸtirmeler ===")
    analyzer.create_advanced_visualizations()

if __name__ == "__main__":
    main()